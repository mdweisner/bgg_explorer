---
title: "BGG Explorer Github"
author: "Michael Weisner and Chana Messinger"
date: "November 2, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE, error=TRUE, warning=FALSE)
```

## Goals
+ K means cluster to recommendation?
+ Lasso penalized prediction of score and other methods

## To Do:
+ Talk to Ben about project (OK!)
+ Figure out what to do with data cleaning (DONE)
+ K Means: Do a plot and a tree visualization
+ + Also ask Ben about the clustering?
+ Run: lasso / ridge, randomforest, BART (Almost DONE)
+ + Pick best one
+ + Show actual examples of the best one
+ + Demonstrate made up examples (ex game with mechanics x and categories y and weight blah blah)
+ Document sections and give some analysis
+ Do more summary statistics and graphs
+ + histograms
+ + missing data
+ + plots
+ Explain a good meopdel metric (ROC, accuracy, precision, and recall or RMSE in our case)
+ Show a baseline model (lm())
+ Modeling process?
+ Carefully design the scope of the project - debugging will take longer than expected


+ Graphs/Exploratory
+ Pick the best model predictor
+ Predict with that model
+ Write about each section

## Libraries
```{r}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(psych)
library(rpart.plot)
library(purrr)
```
## BGG Data
The dataset, bgg_db_1806.csv, is a dataset scraped in June of 2018 from the Boardgame Geek (BGG) website, a repository of boardgames that categorizes and ranks games in conjunction with curating information about the games' basic features. The data was hosted at kaggle.com by user mrpantherson at https://www.kaggle.com/mrpantherson/board-game-data.
Below we can see the basic variables of the dataset:
+ rank: the rank of each boardgame on the Boardgame Geek website. 
+ bgg_url: the game's specific URL
+ game_ID: the unique ID used by Boardgamegeek.com
+ names: the name of the game
+ min_players: the minimum players required to play the game
+ max_players: the maximum players that can play the game
+ avg_time: the average time to play the game in minutes
+ min_time: the minimum time to play the game in minutes
+ max_time: the maximum time to play the game in minutes
+ year: the year the game was released
+ avg_rating: the average user rating score
+ geek_rating: the rating ascribed by the curators of boardgamegeek.com, and the basis of the rank
+ num_votes: the number of user votes on a game
+ image_url: the url address of an image of the boardgame
+ age: the minimum age to play the game
+ mechanic: a list of mechanics the game utilizes as curated by boardgamegeek.com
+ owned: how many users claim to own the game
+ category: a list of themes the game utilizes as curated by boardgamegeek.com
+ designer: the name of the designer
+ weight: the user scored complexity of a game to learn and play
```{r}
games <- read_csv(unzip("~/bgg_explorer/bgg_db_1806.csv.zip"))
describe(games)
```

## Data Cleaning

### Dropping Outliers
We decided to cut some extreme outliers for the following reasons:
+ Of known games, One Night Ultimate Werewolf has the highest specified player count of 75. Some games list 99 players, suggesting an unlimited number of players. But this is simply not true and impractical for this exercise. While they could be recoded as 75 or greater, since there is no actual specification it makes more sense to drop the variables which could skew the results.
+ Similarly, games with 0 players are equally useless, as games need people.
+ There are two boardgames that have an age restriction of 21+ due to alcohol. One game suggested an age of 42, which made it useless for the analysis.
+ Some games were developed hundreds or even thousands of years ago. These are not highly comparable to more modern games. To focus on these, we will focus on games that have occurred from 1970 onward, slightly before the so-called "boardgame renaissance," the resurgence of interest in strategic and complex boardgames that occurred within the last 30 years. 1970 was chosen somewhat arbitrarily, but seemed safe as it includes the vast majority of games in the dataset.
+ There was also one NA row we dropped.
+ Games with time over 1000 minutes were dropped, as these games seemed to imply the idea that you could play one "campaign" over that time, but it's not a reasonable amount of time anyone could sit and play a boardgame. Similarly games with 0 time were dropped.
+ Lastly, games with no category and mechanic were useless for this exercise, as they do, in fact, have mechanics. They were ignored.

```{r}
games_clean <- games
games_clean <- drop_na(games)
outliers <- subset(games, min_players > 75 | max_players > 75 | min_players == 0 |
                     max_players == 0 | weight == 0 | avg_time > 1000 | min_time > 1000 | max_time > 1000 | 
                     avg_time == 0 | min_time == 0 | max_time == 0 | age > 21 | year < 1970 | mechanic == "none" | category == "none")
games_clean <- subset(games_clean, !(game_id %in% outliers$game_id))
summary(games_clean)
```
Out of the original 4999 games, we're left with 4547 games within more acceptable boundaries. While it's poor form to throw out data, these games have little value given the highly skewed things.

### Mechanics
Boardgamegeek.com has a set of generic boardgame mechanics, which it applies to relevant games. The full list can be found here: https://boardgamegeek.com/browse/boardgamemechanic. The dataset put all of these into a single string, which we broke up into individual dummy variables below. Mechanics included physical things like rolling dice or managing a hand of cards, for instance.
### Create Dummy Variables for each mechanic
```{r echo=FALSE}
mech_raw2 <- unlist(strsplit(games_clean$mechanic, ", "))
mech_unique <- unique(mech_raw2)
mech_terms <- sort(mech_unique)

# Add columns for each mechanic type
games_clean[, mech_terms] <- 0

# Update values of dummy mechanics if present in games_clean$mechanic
for(j in 1:length(mech_terms)){
  for(i in 1:length(games_clean$mechanic)){
    games_clean[i, as.character(mech_terms[j])] <- as.numeric(grepl(mech_terms[j], games_clean$mechanic[i]))
  }
  # cat("now at", round(j * 1.96), "% ... ") # this was to get a sense of progress
}
```

### Rename Mechanics
We renamed the mechanics columns for ease of use.
```{r}
library(data.table)
colnames(games_clean) <- tolower(colnames(games_clean))
colnames(games_clean) <- gsub("co-op", "coop", colnames(games_clean)) # odd one out with dash
colnames(games_clean) <- gsub("\\/", " ", colnames(games_clean))
colnames(games_clean) <- gsub("&", "", colnames(games_clean))
colnames(games_clean) <- gsub("-", " ", colnames(games_clean))
colnames(games_clean) <- gsub(" and ", " ", colnames(games_clean))
colnames(games_clean) <- gsub("  ", " ", colnames(games_clean))
colnames(games_clean) <- gsub("  ", " ", colnames(games_clean))
colnames(games_clean) <- gsub(" ", "_", colnames(games_clean))
# colnames(games_clean) <- gsub("none", "no_mech", colnames(games_clean))

mech_old_names <- colnames(select(games_clean, acting:worker_placement))
mech_new_names <- paste0('mech_', mech_old_names)
setnames(games_clean, old = mech_old_names, new = mech_new_names)

head(select(games_clean, mech_acting:mech_worker_placement))
```

### Total Mechanics
```{r}
games_clean <- games_clean %>%
  mutate(mech_total = rowSums(select(games_clean, mech_acting:mech_worker_placement)))
summary(games_clean$mech_total)
```

### Review Mech Stats
```{r}
describe(select(games_clean, mech_acting:mech_worker_placement))
```

### Categories
Boardgamegeek.com also ascribes category values to games. We similarly broke these categories into dummy variables and renamed them for better use. A full list of categories can be found here: https://boardgamegeek.com/browse/boardgamecategory. Categories were more varied than mechanics. Some were thematic, like a game having a "fantasy" or "war" theme, but also had some mechanical concepts like being a card game.

```{r echo=FALSE}
cat_raw2 <- unlist(strsplit(games_clean$category, ", "))
cat_unique <- unique(cat_raw2)
cat_terms <- sort(cat_unique)

# Add columns for each mechanic type
games_clean[, cat_terms] <- 0

# Update values of dummy mechanics if present in games_clean$mechanic
for(j in 1:length(cat_terms)){
  for(i in 1:length(games_clean$category)){
    games_clean[i, as.character(cat_terms[j])] <- as.numeric(grepl(cat_terms[j], games_clean$category[i]))
  }
  # cat("now at", round(j * 1.19), "% ... ") # this was used to gauge progress of the function
}
```
### Rename Categories
The categories were similarly named here.
```{r}
colnames(games_clean) <- tolower(colnames(games_clean))
colnames(games_clean) <- gsub("co-op", "coop", colnames(games_clean)) # odd one out with dash
colnames(games_clean) <- gsub("\\/", " ", colnames(games_clean))
colnames(games_clean) <- gsub("&", "", colnames(games_clean))
colnames(games_clean) <- gsub("-", " ", colnames(games_clean))
colnames(games_clean) <- gsub(" and ", " ", colnames(games_clean))
colnames(games_clean) <- gsub("  ", " ", colnames(games_clean))
colnames(games_clean) <- gsub("  ", " ", colnames(games_clean))
colnames(games_clean) <- gsub(" ", "_", colnames(games_clean))
colnames(games_clean) <- gsub("\\'", "", colnames(games_clean))

cat_old_names <- colnames(select(games_clean, abstract_strategy:zombies))
cat_new_names <- paste0('cat_', cat_old_names)
setnames(games_clean, old = cat_old_names, new = cat_new_names)
head(games_clean)
head(select(games_clean, cat_abstract_strategy:cat_zombies))
```

### Total Categories
```{r}
games_clean <- games_clean %>%
  mutate(cat_total = rowSums(select(games_clean, cat_abstract_strategy:cat_zombies)))
summary(games_clean$cat_total)
```
### Save games_clean
This was purely to save the cleaned dataset for future use.
```{r}
write_csv(games_clean, "games_clean.csv")
```

### Review Category Stats
A potential issue was that many game mechanics and categories were basically unused. The amount of predictive power of these would most likely be minimal.
```{r}
describe(select(games_clean, cat_abstract_strategy:cat_zombies))
```

### Review Mechanic Stats
```{r}
describe(select(games_clean, mech_acting:mech_worker_placement))
```

### Check Some Stats!
Here we looked at a popular game series, the Pandemic series, which are games about stopping a global outbreak of disease. Interestingly the games vary somewhat considerably in terms of players and time.
```{r}
filter(games_clean, grepl("pandemic", ignore.case = TRUE, names))
```

### Times
The games had an average of ~90 minutes to play, but a median of 60 minutes.
```{r}
describe(games_clean$avg_time)
```

## Plotting
We explored the distributions of some scores.

### Time

```{r}
gg <- ggplot(data = games_clean, aes(x = min_time)) +
  geom_histogram(binwidth = 15) +
  geom_smooth(aes(x = min_time, y = geek_rating*100), size = 1.5) +
  scale_y_continuous(sec.axis = sec_axis(~./100, name = "Average Geek Rating")) +
  xlim(0, 450)
gg
```


### Geek Rating by Minimum & Maximum Time

```{r}
gg <- ggplot(data = games_clean, aes(x = min_time, y = geek_rating, col = max_time, size = weight))
gg <- gg + geom_jitter(alpha = 0.3)
gg <- gg + geom_smooth()
gg
```
```{r}
gg <- ggplot(data = games_clean, aes(x = max_time, y = geek_rating, col = min_time, size = weight))
gg <- gg + geom_jitter(alpha = 0.3)
gg <- gg + geom_smooth()
gg
```

### Geek Rating by Weight
```{r}
gg <- ggplot(data = games_clean, aes(x = weight, y = geek_rating, size = avg_time))
gg <- gg + geom_jitter(alpha = 0.3)
gg <- gg + geom_smooth()
gg
```



### Ownership by Geek Rating
```{r}
gg <- ggplot(data = games_clean, aes(x = geek_rating, y = owned, col = avg_time))
gg <- gg + geom_point(alpha = 0.6)
gg <- gg + geom_smooth()
gg

```
## Quick Load of Cleaned Data
```{r}
games_clean <- read_csv("~/bgg_explorer/games_clean.csv") # added to start here without rerunning everything
```

# K means clustering

To investigate our dataset further, we used unsupervised learning in the form of k-means clustering. To get a general sense of our data, we looked at five clusters. Then, with an eye towards making recommendations, we chose 500 centers so that each cluster would have approximately 9 games (4547 observations split among 500 clusters). 

For exploration purposes, I have plotted the same 5 cluster k-means with geek rating on the y axis and three completely different variables (avg_time - average time, mech_dice_rolling - whether dice rolling is a mechanic in the game and weight - complexity of the game).

```{r}
set.seed(12345)
games_numeric_2 <- select(games_numeric, -c("game_id", "rank", "avg_rating", "num_votes", "min_time", "max_time"))

km_games1 <- kmeans(games_numeric_2, centers = 5, nstart = 25)
with(games_numeric_2, plot(avg_time, geek_rating, col = km_games1$cluster, pch = 20, main = "Geek Rating against Average Playing Time"))

with(games_numeric_2, plot(mech_dice_rolling, geek_rating, col = km_games1$cluster, pch = 20, main = "Geek Rating against Whether Dice Rolling Mechanic is Present"))

with(games_numeric_2, plot(weight, geek_rating, col = km_games1$cluster, pch = 20, main = "Geek Rating against Weight (complexity"))

```
This rudimentary clustering gives us a sense that geek rating (which will be our outcome variable of interest later), is an important metric upon which to cluster, since all three plots above are broadly stratified vertically and not horizontally on either of any of the variables picked.


```{r, cache = TRUE}
km_games <- kmeans(games_numeric_2, centers = 500, nstart = 25)
with(games_numeric_2, plot(weight, geek_rating, col = km_games$cluster, pch = 20, main = "Geek Rating by Weight with 500 clusters"))
```
Unsurprisingly with 500 clusters, no obvious patterns emerge. 

We now use k-means to make a recommendation function. To that end, we have a helper function that looks up parts of games for the user so that they can get the full name with correct cases, which is necessary in the following functions. 

## Lookup function
```{r}
lookup <- function(gamenamepart){
  games_clean$names[str_detect(games_clean$names, fixed(gamenamepart, ignore_case = T))]
}

#Example
lookup("tiny epic")
```

The basic recommendation function simply accepts a game name and returns the other games that were in its cluster. The default is "Gloomhaven", the currently top-rated game on Board Game Geek. Different games have different numbers of recommendations based on the size of the cluster, which is fair if we understand that some games have fewer others like them. Any two inputs from the same cluster will give the same output (except for not recommending itself). 

## Basic Recommendations
```{r}
recommendation <- function(gamename = "Gloomhaven"){
  detect <- str_which(games_clean$names, paste0("^", gamename, "$"))
  cluster <- km_games$cluster[detect]
  recs <- games_clean$names[which(km_games$cluster == cluster)]
  recs <- recs[-which(recs == gamename)]
  return(recs)
}

#Example
recommendation("Hanabi")
```

The more sophisticated recommendation function runs independent kmeans clusters and gives each game a point for every time it appears in the same cluster as the input game. Then it returns a dataframe with the top results by score.

## Recommendation Score
```{r}
recommendation2 <- function(gamename = "Gloomhaven", runs = 5, number = 5){
  # Create a new numeric variable called Score (which gives it a value of 1, so that is subtracted out)
  Score <- as.data.frame(cbind(games_clean$names, "score" = 0))
  Score$score <- as.numeric(Score$score) - 1
  
  
  # Stop the game from recommending itself
  detect <- str_which(games_clean$names, paste0("^", gamename, "$"))[1]
Score[detect, ] <- NA 

set.seed(12345)

# Loop for kmeans clustering with 500 clusters. Because of the number of clusters, it can take some time to run.
  for (i in 1:runs){
      km_games <- kmeans(games_numeric_2, centers = 500, nstart = 25)
      cluster <- km_games$cluster[detect]
      Score$score[which(km_games$cluster == cluster)] <- Score$score[which(km_games$cluster == cluster)] + 1
  }
#Create the recommendation table and return it
  recs <- head(arrange(Score, desc(score)), number)
  colnames(recs) <- c("Game", paste0("Score up to ",runs))
  return(recs)
}

#Example
recommendation2("Gloom", 10, 10)
```

# Supervised Learning

Given the large number of mechanical and thematic variables, we were interested in trying to predict the games' "geek rating" score by modeling these various variables. To simplify the model we dropped non-numeric variables like urls, names, and designers which would arguably not have value in the scoring as well as obviously highly colinear variables like rank, which is purely an ordering of geek rating.

## Multiple Cores
```{r}
library(doMC)
registerDoMC(parallel::detectCores())
```

## Subset
Here we dropped variables we believed would be confounding or unhelpful.
```{r}
games_clean_sub <- games_clean %>%
  select(min_players:cat_total, -num_votes, -image_url, -mechanic, -category, -designer, -owned, -avg_rating, -mech_total, -cat_total)
```


## Linear Model
We ran an OLS model on the subset.
```{r}
lm1 <- lm(geek_rating ~ ., data = games_clean_sub)
summary(lm1)
```

## Training
```{r}
library(caret)
set.seed(12345)
ctrl <- trainControl(method = "cv", number = 10)
in_train <- createDataPartition(y = games_clean_sub$geek_rating,
                                p = 3 / 4, list = FALSE)
str(in_train)
games_clean_training <- games_clean_sub[ in_train, ]
games_clean_testing  <- games_clean_sub[-in_train, ]
```

## OLS Model in Machine Learning Context
Step 1: Solve an optimization problem using the pre-processed training data
```{r}
set.seed(12345)
fit_lm <- train(geek_rating ~ ., data = games_clean_training, method = "lm")
fit_lm
```

Step 2: Predict in the testing data
The predict lm throws out data it thinks is useless.
```{r}
y_hat_lm <- predict(fit_lm, newdata = games_clean_testing)
```

Step 3: Evaluate
```{r}
defaultSummary(data.frame(obs = games_clean_testing$geek_rating, pred = y_hat_lm))
```

Our testing data using the OLS model actually had as lightly better RMSE score than in the training data of ~0.4368. This is the main score of interest for us to compare across our models.

## Lasso
A penalized regression seemed logical step after an OLS model. 

### Initialize Grid
```{r}
set.seed(12345)
lassoGrid <- expand.grid(.lambda = seq(.05, 1, length = 10),
                        .fraction = seq(.05, 1, length = 10))
# head(lassoGrid, n = 20)
```

```{r cache=TRUE}
set.seed(12345)
lasso <- train(geek_rating ~ ., data = games_clean_training, method = "enet", 
               trControl = ctrl, tuneGrid = lassoGrid, linout = TRUE)
```

### Prediction
```{r}
y_hat_lasso <- predict(lasso, newdata = games_clean_testing)
# mean( (games_clean_testing$geek_rating - y_hat) ^ 2 )
defaultSummary(data.frame(obs = games_clean_testing$geek_rating, pred = y_hat_lasso))
```
Lasso penalized prediction is just MARGINALLY better than the OLS model.

## Flam
The FLAM model wouldn't complete on a full dataset so I created a sample subset.
```{r, cache = true}
library(flam)
set.seed(12345)
games_clean_training_sub <- games_clean_training[sample(1:nrow(games_clean_training), 1000, replace=FALSE), ]
X <- model.matrix(geek_rating ~ ., data = games_clean_training_sub)
fit_flam <- flamCV(x = X, y = games_clean_training_sub$geek_rating)
yhat_flam <- predict(fit_flam$flam.out,
                     new.x = model.matrix(geek_rating ~ ., data = games_clean_testing),
                     lambda = fit_flam$lambda.cv, alpha = fit_flam$alpha)
defaultSummary(data.frame(obs = games_clean_testing$geek_rating, pred = yhat_flam))
```


## Random Forest
 
```{r, cache=TRUE}
# library(randomForest)
# rf_grid <- data.frame(.mtry = 2:(ncol(games_clean_training) - 1L)) # across all data
set.seed(12345)
ctrl <- trainControl(method = "cv", number = 3)
rf_grid <- data.frame(.mtry = 2:(ncol(games_clean_training) - 1L)) # I had to reduce this to get it to run, default is 2/3, maybe change knot size? 
rf_out <- train(geek_rating ~ ., data = games_clean_training, method = "rf",
             trControl = ctrl, tuneGrid = rf_grid, 
             ntrees = 10, importance = TRUE) # I tried cutting fewer trees
```
Variables of Importance
```{r}
varImp(rf_out)
```

Compare
```{r, cache=TRUE}
y_hat_rf <- predict(rf_out, newdata = games_clean_testing)
defaultSummary(data.frame(obs = games_clean_testing$geek_rating, 
                          pred = exp(y_hat_rf)))
```


## BARTs
```{r cache=TRUE}
library(BART)
set.seed(12345)
X_train <- model.matrix(geek_rating ~ ., data = games_clean_training)
X_test <- model.matrix(geek_rating ~ ., data = games_clean_testing)
out_bart <- mc.wbart(X_train, y = games_clean_training$geek_rating, X_test,
                mc.cores = parallel::detectCores())
defaultSummary(data.frame(obs = games_clean_testing$geek_rating,
                          pred = exp(out_bart$yhat.test.mean)))
```


## Neural Net
```{r, cache = true}
library(e1071)
set.seed(12345)
nnet_grid <- expand.grid(.decay = c(0, 0.01, 0.1),
                        .size = c(1:10))

ctrl <- trainControl(method = "cv", number = 10)

out_nn <- train(geek_rating ~ ., data = games_clean_training, method = "nnet",
             trControl = ctrl, tuneGrid = nnet_grid,
             trace = FALSE, linout = TRUE,
            mc.cores = parallel::detectCores())
defaultSummary(data.frame(obs = games_clean_testing$geek_rating,
                          pred = predict(out_nn, newdata = games_clean_testing)))

```
```{r}
varImp(out_nn)
```

## Prediction of geek rating without and with a data frame returned
In the end, our Ordinary Least Squares Linear Regression had the lowest Root Mean Squared Error (RMSE), so it is what we are using to make predictions of the geek rating from the other variables. 

We predict across the original cleaned dataset and add in a new predicted column, to compare against the actual geek rating.

### Get Game Prediction
```{r}
lasso_pred_full_data <- predict(lasso, newdata = games_clean_sub)
games_clean_preds <- games_clean
games_clean_preds$predicted_rating <- lasso_pred_full_data 
games_clean_preds <- games_clean_preds[, c(1, 4, 12, 157, 2, 3, 5:11, 13:156)]
```

```{r}
head(games_clean_preds[, 1:4], n = 10)
```

